# Hypothesis Class Restrictions

If the hypothesis class si too powerful, variance increases.

We want to find the variance-bias tradeoff area where our hypothesis class is effective.

1. We can select features, but this won't work for some reason
2. We can consider all features, and restrict $\mathcal{H}$ in some smart way

If we allow any point in the space to be a member of our hypothesis class, then we can can restrict perhaps the length of either weight vector, say to be on the unit circle/sphere/hypersphere.

so this expands to $$ \mathcal{H}_2 = \{h(x) = w^Tx: w\in \mathbb{R}^{d+1}\cdot ||w||_1 \leq c_1\} \\ \mathcal{H}_2 = \{h(x) = w^Tx: w\in \mathbb{R}^{d+1}\cdot ||w||_2 \leq c_2\} $$

we can optimize this by minimizing $w$: $$ \min_w \frac{1}{2M} ||Xw-t||^2 $$ subject to $||w||^2_2\leq c_2$

Recall that L1 loss only makes use of one bound. L2 loss, on the other hand, allows us to use a 2-variable bouind in $c_1$ and $c_2$. L2 loss now provides a more accurate estimation of the loss.

The L2 may be far from the optimum, but the loss will be smaller when compared to the L1. When we deal with the loss, we will always use L2 loss.

Man I need to study this.

Sharp points or edges (hyper edges) mean that we are likely to be trapped on the points and edges. L1 regularization results in a sparse solution.

Sharp points = sparse

> Hard Constraints are equivalent to Soft Penalties for convex optimization
